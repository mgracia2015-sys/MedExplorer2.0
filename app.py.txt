%%writefile app.py
import streamlit as st
from Bio import Entrez
from datetime import datetime, timedelta
import time
import http.client
import pandas as pd

MAX_TOTAL_ARTICLES = 10000
MAX_PER_KEYWORD = 9500

def safe_read(handle):
    try:
        return Entrez.read(handle)
    except http.client.IncompleteRead as e:
        return Entrez.read(e.partial)

def is_ukrainian_affiliation(affil_text):
    affil_lower = affil_text.lower()
    return any(word in affil_lower for word in [
        "ukraine", "ukrainian", "—É–∫—Ä–∞—ó–Ω–∞", "—É–∫—Ä–∞—ó–Ω–∏", "—É–∫—Ä–∞—ó–Ω—ñ"
    ])

def has_joint_publications(author_name, target_author, email):
    Entrez.email = email
    query = f"{author_name}[Author] AND {target_author}[Author]"
    try:
        handle = Entrez.esearch(db="pubmed", term=query, retmax=0)
        record = Entrez.read(handle)
        handle.close()
        return int(record["Count"]) > 0
    except:
        return False

def count_author_articles(author_name, email):
    Entrez.email = email
    today = datetime.now()
    five_years_ago = today - timedelta(days=5 * 365)
    date_range = f"{five_years_ago.strftime('%Y/%m/%d')}[PDat] : {today.strftime('%Y/%m/%d')}[PDat]"
    query = f"{author_name}[Author] AND {date_range}"
    try:
        handle = Entrez.esearch(db="pubmed", term=query, retmax=0)
        record = Entrez.read(handle)
        handle.close()
        return int(record["Count"])
    except:
        return 0

def summarize_final_candidates(author_dict, target_author, email):
    final = []
    for key, data in author_dict.items():
        name = data["name"]
        affil = data["affil"]
        orcid = data["orcid"]
        keywords = sorted(set(data["keywords"]))
        if has_joint_publications(name, target_author, email):
            continue
        count = count_author_articles(name, email)
        final.append({
            "–ê–≤—Ç–æ—Ä": name,
            "–ê—Ñ—ñ–ª—ñ–∞—Ü—ñ—è": affil,
            "ORCID": orcid,
            "–°—Ç–∞—Ç–µ–π –∑–∞ 5 —Ä–æ–∫—ñ–≤": count,
            "–ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞": ", ".join(keywords)
        })
    df = pd.DataFrame(final)
    df = df.sort_values(by="–°—Ç–∞—Ç–µ–π –∑–∞ 5 —Ä–æ–∫—ñ–≤", ascending=False)
    return df

def get_keyword_counts(keywords, email, date_range):
    Entrez.email = email
    Entrez.tool = "UkrainianReviewerFinder"
    keyword_counts = {}
    for kw in set(keywords):
        handle = Entrez.esearch(db="pubmed", term=f"{kw}[All Fields] AND {date_range}", retmax=0)
        record = Entrez.read(handle)
        handle.close()
        count = int(record["Count"])
        keyword_counts[kw] = count
    return keyword_counts

def search_pubmed_articles(email, author_lastname, affiliation):
    Entrez.email = email
    Entrez.tool = "UkrainianReviewerFinder"

    today = datetime.now()
    five_years_ago = today - timedelta(days=5 * 365)
    date_range = f"{five_years_ago.strftime('%Y/%m/%d')}[PDat] : {today.strftime('%Y/%m/%d')}[PDat]"
    query = f"{author_lastname}[Author] AND {affiliation}[Affiliation] AND {date_range}"

    handle = Entrez.esearch(db="pubmed", term=query, retmax=1000)
    record = Entrez.read(handle)
    handle.close()
    id_list = record["IdList"]

    keywords = []
    batch_size = 20
    for i in range(0, len(id_list), batch_size):
        batch_ids = id_list[i:i+batch_size]
        time.sleep(0.5)
        try:
            fetch_handle = Entrez.efetch(db="pubmed", id=batch_ids, rettype="medline", retmode="text")
            article_data = fetch_handle.read()
            fetch_handle.close()
        except Exception as e:
            st.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ –¥–∞–Ω–∏—Ö –¥–ª—è batch {i}: {e}")
            continue

        for line in article_data.split("\n"):
            if line.startswith("OT  -"):
                keyword = line.replace("OT  - ", "").strip()
                keywords.append(keyword)

    return keywords, len(id_list)

def find_authors_by_keywords(keywords, email, target_author):
    Entrez.email = email
    Entrez.tool = "UkrainianReviewerFinder"

    today = datetime.now()
    five_years_ago = today - timedelta(days=5 * 365)
    date_range = f"{five_years_ago.strftime('%Y/%m/%d')}[PDat] : {today.strftime('%Y/%m/%d')}[PDat]"

    keyword_counts = get_keyword_counts(keywords, email, date_range)
    sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1])

    author_dict = {}
    total_articles_processed = 0

    for keyword, count in sorted_keywords:
        if total_articles_processed + min(count, MAX_PER_KEYWORD) > MAX_TOTAL_ARTICLES:
            st.warning(f"‚è∏Ô∏è –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –∫–ª—é—á–æ–≤–∏–º —Å–ª–æ–≤–æ–º **{keyword}**, —â–æ–± –Ω–µ –ø–µ—Ä–µ–≤–∏—â–∏—Ç–∏ –ª—ñ–º—ñ—Ç 10 000 —Å—Ç–∞—Ç–µ–π.")
            ukrainian_authors = {
                k: v for k, v in author_dict.items() if is_ukrainian_affiliation(v["affil"])
            }
            df = summarize_final_candidates(ukrainian_authors, target_author, email)
            st.success(f"‚úÖ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –∞–≤—Ç–æ—Ä—ñ–≤ –±–µ–∑ —Å–ø—ñ–ª—å–Ω–∏—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π: {len(df)}")
            st.dataframe(df.reset_index(drop=True), use_container_width=True)
            total_articles_processed = 0
            time.sleep(5)

        max_to_process = min(count, MAX_PER_KEYWORD)
        st.markdown(f"### üîç –ö–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ: **{keyword}** ‚Äî `{count}` —Å—Ç–∞—Ç–µ–π (–æ–±—Ä–æ–±–ª—è—î–º–æ –¥–æ {max_to_process})")
        query = f"{keyword}[All Fields] AND {date_range}"
        handle = Entrez.esearch(db="pubmed", term=query, usehistory="y", retmax=0)
        record = Entrez.read(handle)
        handle.close()
        webenv = record["WebEnv"]
        query_key = record["QueryKey"]
        batch_size = 50
        retrieved = 0

        while retrieved < max_to_process:
            time.sleep(0.5)
            try:
                fetch_handle = Entrez.efetch(
                    db="pubmed",
                    rettype="xml",
                    retmode="xml",
                    retstart=retrieved,
                    retmax=batch_size,
                    query_key=query_key,
                    webenv=webenv
                )
                records = safe_read(fetch_handle)
                fetch_handle.close()
            except Exception as e:
                st.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ batch {retrieved}: {e}")
                break

            if "PubmedArticle" not in records:
                break
            for article in records["PubmedArticle"]:
                pub_date = article["MedlineCitation"].get("DateCompleted", {})
                year = int(pub_date.get("Year", 1900))
                month = int(pub_date.get("Month", 1))
                day = int(pub_date.get("Day", 1))
                article_date = datetime(year, month, day)

                if "AuthorList" not in article["MedlineCitation"]["Article"]:
                    continue
                for author in article["MedlineCitation"]["Article"]["AuthorList"]:
                    name = ""
                    if "LastName" in author and "Initials" in author:
                        name = f"{author['LastName']} {author['Initials']}"
                    elif "CollectiveName" in author:
                        name = author["CollectiveName"]
                    affil = ""
                    if "AffiliationInfo" in author:
                        affil_blocks = author["AffiliationInfo"]
                        if affil_blocks and "Affiliation" in affil_blocks[0]:
                            affil = affil_blocks[0]["Affiliation"]
                    orcid = ""
                    if "Identifier" in author:
                        for ident in author["Identifier"]:
                            if ident.attributes.get("Source") == "ORCID":
                                orcid = str(ident)
                    key = orcid if orcid else name
                    if key not in author_dict or article_date > author_dict[key]["latest_date"]:
                        author_dict[key] = {
                            "name": name,
                            "affil": affil,
                            "orcid": orcid,
                            "latest_date": article_date,
                            "keywords": []
                        }
                    author_dict[key]["keywords"].append(keyword)
            retrieved += batch_size
            total_articles_processed += batch_size

    return author_dict
# üé® Streamlit UI
st.set_page_config(page_title="PubMed –ê–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä", layout="centered")
st.title("üî¨ –ü–æ—à—É–∫ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ —Ç–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç—ñ–≤")
st.markdown("–ö—Ä–æ–∫ 1: –∑–Ω–∞–π—Ç–∏ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –∞–≤—Ç–æ—Ä–∞. –ö—Ä–æ–∫ 2: –∑–Ω–∞–π—Ç–∏ –∞–≤—Ç–æ—Ä—ñ–≤ —Å—Ç–∞—Ç–µ–π –∑ —Ç–∏–º–∏ –∂ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏. –ö—Ä–æ–∫ 3: –≤–∏–±—Ä–∞—Ç–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç—ñ–≤ –±–µ–∑ —Å–ø—ñ–ª—å–Ω–∏—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π.")

email = st.text_input("üìß –í–∞—à email –¥–ª—è –¥–æ—Å—Ç—É–ø—É –¥–æ PubMed")
lastname = st.text_input("üë§ –ü—Ä—ñ–∑–≤–∏—â–µ –∞–≤—Ç–æ—Ä–∞")
affiliation = st.text_input("üè¢ –ê—Ñ—ñ–ª—ñ–∞—Ü—ñ—è –∞–≤—Ç–æ—Ä–∞")

if "keywords" not in st.session_state:
    st.session_state.keywords = []
if "author_dict" not in st.session_state:
    st.session_state.author_dict = {}

# –ö–†–û–ö 1: –ü–æ—à—É–∫ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
if st.button("üîç –ö—Ä–æ–∫ 1: –ó–Ω–∞–π—Ç–∏ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –∞–≤—Ç–æ—Ä–∞"):
    if not email or not lastname or not affiliation:
        st.warning("–ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–ø–æ–≤–Ω—ñ—Ç—å email, –ø—Ä—ñ–∑–≤–∏—â–µ —Ç–∞ –∞—Ñ—ñ–ª—ñ–∞—Ü—ñ—é –∞–≤—Ç–æ—Ä–∞.")
    else:
        with st.spinner("–ü–æ—à—É–∫ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤..."):
            keywords, count = search_pubmed_articles(email, lastname, affiliation)
            st.session_state.keywords = keywords
            st.session_state.search_attempted = True  # üîë –ø–æ–∑–Ω–∞—á–∞—î, —â–æ –ø–æ—à—É–∫ –±—É–≤

        st.success(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {count} —Å—Ç–∞—Ç–µ–π.")

# üîë –í–∏–≤—ñ–¥ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤, —è–∫—â–æ –∑–Ω–∞–π–¥–µ–Ω–æ
if st.session_state.get("keywords"):
    st.subheader("üîë –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞:")
    for kw in sorted(set(st.session_state.keywords)):
        st.markdown(f"- {kw}")

# ‚úçÔ∏è –ü—Ä–æ–ø–æ–∑–∏—Ü—ñ—è —Ä—É—á–Ω–æ–≥–æ –≤–≤–µ–¥–µ–Ω–Ω—è ‚Äî —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –ø–æ—à—É–∫ –±—É–≤ —ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ
elif st.session_state.get("search_attempted") and not st.session_state.keywords:
    st.warning("‚ùå –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –í–∏ –º–æ–∂–µ—Ç–µ –≤–≤–µ—Å—Ç–∏ —ó—Ö –≤—Ä—É—á–Ω—É –Ω–∏–∂—á–µ.")
    manual_input = st.text_input("‚úçÔ∏è –í–≤–µ–¥—ñ—Ç—å –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –≤—Ä—É—á–Ω—É (—á–µ—Ä–µ–∑ –∫—Ä–∞–ø–∫—É –∑ –∫–æ–º–æ—é):")
    if manual_input:
        manual_keywords = [kw.strip() for kw in manual_input.split(";") if kw.strip()]
        st.session_state.keywords = manual_keywords
        st.success(f"‚úÖ –í–≤–µ–¥–µ–Ω–æ {len(manual_keywords)} –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –≤—Ä—É—á–Ω—É.")
        st.subheader("üîë –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞:")
        for kw in manual_keywords:
            st.markdown(f"- {kw}")

# –ö–†–û–ö 2: –ü–æ—à—É–∫ –∞–≤—Ç–æ—Ä—ñ–≤
if st.button("üîé –ö—Ä–æ–∫ 2: –ó–Ω–∞–π—Ç–∏ –∞–≤—Ç–æ—Ä—ñ–≤ –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏"):
    if not st.session_state.keywords or not email or not lastname:
        st.warning("–ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–ø–æ–≤–Ω—ñ—Ç—å email, –ø—Ä—ñ–∑–≤–∏—â–µ –∞–≤—Ç–æ—Ä–∞ —Ç–∞ –≤–∏–∫–æ–Ω–∞–π—Ç–µ –∫—Ä–æ–∫ 1.")
    else:
        with st.spinner("–ü–æ—à—É–∫ –∞–≤—Ç–æ—Ä—ñ–≤..."):
            author_dict = find_authors_by_keywords(st.session_state.keywords, email, lastname)
            st.session_state.author_dict = author_dict
        st.success(f"‚úÖ –ó–∞–≥–∞–ª–æ–º –∑–Ω–∞–π–¥–µ–Ω–æ {len(author_dict)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∞–≤—Ç–æ—Ä—ñ–≤.")

# –ö–†–û–ö 3: –í–∏–≤—ñ–¥ —Ñ—ñ–Ω–∞–ª—å–Ω–æ—ó —Ç–∞–±–ª–∏—Ü—ñ
if st.button("üá∫üá¶ –ö—Ä–æ–∫ 3: –ü–æ–∫–∞–∑–∞—Ç–∏ —Ñ—ñ–Ω–∞–ª—å–Ω—É —Ç–∞–±–ª–∏—Ü—é —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç—ñ–≤"):
    if not st.session_state.author_dict or not email or not lastname:
        st.warning("–°–ø–æ—á–∞—Ç–∫—É –≤–∏–∫–æ–Ω–∞–π—Ç–µ –∫—Ä–æ–∫ 2.")
    else:
        ukrainian_authors = {
            k: v for k, v in st.session_state.author_dict.items() if is_ukrainian_affiliation(v["affil"])
        }
        df = summarize_final_candidates(ukrainian_authors, lastname, email)
        st.success(f"‚úÖ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –∞–≤—Ç–æ—Ä—ñ–≤ –±–µ–∑ —Å–ø—ñ–ª—å–Ω–∏—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π: {len(df)}")
        st.dataframe(df.reset_index(drop=True), use_container_width=True)

        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–∞–±–ª–∏—Ü—é –≤ CSV",
            data=csv,
            file_name="ukrainian_reviewers.csv",
            mime="text/csv"
        )

# üß© –ö–†–û–ö 4: –ü–æ—à—É–∫ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏—Ö –æ–ø–æ–Ω–µ–Ω—Ç—ñ–≤ –¥–ª—è –¥–∏—Å–µ—Ä—Ç–∞–Ω—Ç–∞
st.markdown("---")
st.header("üß© –ö—Ä–æ–∫ 4: –ü–æ—à—É–∫ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏—Ö –æ–ø–æ–Ω–µ–Ω—Ç—ñ–≤ –¥–ª—è –¥–∏—Å–µ—Ä—Ç–∞–Ω—Ç–∞")

uploaded_file = st.file_uploader("üìÑ –ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ CSV-—Ñ–∞–π–ª –∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏", type="csv")

with st.expander("üë• –í–≤–µ–¥—ñ—Ç—å –¥–∞–Ω—ñ 4 –≤—á–µ–Ω–∏—Ö"):
    col1, col2 = st.columns(2)
    head_name = col1.text_input("üë§ –ü—Ä—ñ–∑–≤–∏—â–µ –≥–æ–ª–æ–≤–∏ —Ä–∞–¥–∏")
    head_affil = col2.text_input("üè¢ –ê—Ñ—ñ–ª—ñ–∞—Ü—ñ—è –≥–æ–ª–æ–≤–∏ —Ä–∞–¥–∏")

    supervisor_name = col1.text_input("üë§ –ü—Ä—ñ–∑–≤–∏—â–µ –∫–µ—Ä—ñ–≤–Ω–∏–∫–∞")
    supervisor_affil = col2.text_input("üè¢ –ê—Ñ—ñ–ª—ñ–∞—Ü—ñ—è –∫–µ—Ä—ñ–≤–Ω–∏–∫–∞")

    reviewer1_name = col1.text_input("üë§ –ü—Ä—ñ–∑–≤–∏—â–µ —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç–∞ 1")
    reviewer1_affil = col2.text_input("üè¢ –ê—Ñ—ñ–ª—ñ–∞—Ü—ñ—è —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç–∞ 1")

    reviewer2_name = col1.text_input("üë§ –ü—Ä—ñ–∑–≤–∏—â–µ —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç–∞ 2")
    reviewer2_affil = col2.text_input("üè¢ –ê—Ñ—ñ–ª—ñ–∞—Ü—ñ—è —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç–∞ 2")

if st.button("üîç –ó–Ω–∞–π—Ç–∏ –æ–ø–æ–Ω–µ–Ω—Ç—ñ–≤"):
    if not uploaded_file:
        st.warning("–ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ CSV-—Ñ–∞–π–ª –∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏.")
    elif not lastname or not email:
        st.warning("–°–ø–æ—á–∞—Ç–∫—É –∑–∞–ø–æ–≤–Ω—ñ—Ç—å email —Ç–∞ –ø—Ä—ñ–∑–≤–∏—â–µ –∞–≤—Ç–æ—Ä–∞ –Ω–∞ –ö—Ä–æ—Ü—ñ 1.")
    else:
        df = pd.read_csv(uploaded_file)
        st.info(f"üìä –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤.")

        exclude_names = [lastname, head_name, supervisor_name, reviewer1_name, reviewer2_name]
        exclude_names = [name for name in exclude_names if name]

        filtered_rows = []
        with st.spinner("üîé –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å–ø—ñ–ª—å–Ω–∏—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π..."):
            for _, row in df.iterrows():
                candidate_name = row["–ê–≤—Ç–æ—Ä"]
                if any(has_joint_publications(candidate_name, other, email) for other in exclude_names):
                    continue
                filtered_rows.append(row)

        if filtered_rows:
            result_df = pd.DataFrame(filtered_rows)
            st.success(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(result_df)} –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏—Ö –æ–ø–æ–Ω–µ–Ω—Ç—ñ–≤.")
            st.dataframe(result_df.reset_index(drop=True), use_container_width=True)

            csv = result_df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–∞–±–ª–∏—Ü—é –æ–ø–æ–Ω–µ–Ω—Ç—ñ–≤ –≤ CSV",
                data=csv,
                file_name="potential_opponents.csv",
                mime="text/csv"
            )
        else:
            st.warning("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –±–µ–∑ —Å–ø—ñ–ª—å–Ω–∏—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π.")